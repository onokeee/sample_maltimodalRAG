"""
ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
ç”»åƒã‚’æ–‡ç« ä¸­ã«åŸ‹ã‚è¾¼ã‚“ã§å›ç­”ã‚’ç”Ÿæˆ
"""
import base64
import io
import json
from typing import List, Dict, Any
from PIL import Image
import streamlit as st

from core.openai_client import build_chat_messages, call_chat_api
from utils.logger import get_logger

logger = get_logger()


def image_to_base64(image: Image.Image) -> str:
    """PIL Imageã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str


def create_multimodal_prompt(query: str, nodes: List, image_cache) -> tuple:
    """
    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’çµ„ã¿åˆã‚ã›ã¦ã€GPT-4 Visionã«é€ä¿¡
    """
    text_parts = []
    image_documents = []
    
    text_parts.append(f"è³ªå•: {query}\n\n")
    text_parts.append("ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n")
    text_parts.append("ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€ç”»åƒã®å†…å®¹ã‚’å‚ç…§ã—ã¦ã€æ–‡ç« ä¸­ã«ã€Œ[ç”»åƒ1]ã€ã€Œ[ç”»åƒ2]ã€ã®ã‚ˆã†ã«ç•ªå·ã§è¨€åŠã—ã¦ãã ã•ã„ã€‚\n\n")
    
    image_counter = 1
    
    for idx, node in enumerate(nodes):
        # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†
        text_parts.append(f"ã€ã‚½ãƒ¼ã‚¹ {idx + 1}ã€‘")
        text_parts.append(f"ãƒ•ã‚¡ã‚¤ãƒ«: {node.metadata.get('file_name', 'Unknown')}")
        text_parts.append(f"ãƒšãƒ¼ã‚¸: {node.metadata.get('page', '?')}")
        text_parts.append(f"\n{node.text}\n")
        
        # ç”»åƒéƒ¨åˆ†
        if 'image_ids' in node.metadata:
            try:
                image_ids_str = node.metadata['image_ids']
                if isinstance(image_ids_str, str):
                    image_ids = json.loads(image_ids_str)
                else:
                    image_ids = image_ids_str
                
                for image_id in image_ids:
                    cached_data = image_cache.get_image(image_id)
                    if cached_data:
                        image = cached_data["image"]
                        metadata = cached_data["metadata"]
                        
                        # ç”»åƒæƒ…å ±ã‚’è¿½åŠ 
                        text_parts.append(f"\n[ç”»åƒ{image_counter}]: {metadata.get('file_name')} - Page {metadata.get('page')}")
                        
                        # ç”»åƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
                        image_documents.append({
                            "image": image,
                            "metadata": metadata,
                            "number": image_counter
                        })
                        
                        image_counter += 1
            except Exception as e:
                logger.warning(f"Failed to load images for node: {e}")
        
        text_parts.append("\n---\n")
    
    text_parts.append("\nå›ç­”ã®éš›ã¯ã€é–¢é€£ã™ã‚‹ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€Œ[ç”»åƒ1]ã€ã®ã‚ˆã†ã«ç•ªå·ã§è¨€åŠã—ã¦ãã ã•ã„ã€‚")
    text_parts.append("ç”»åƒã®å†…å®¹ã‚’å‚ç…§ã—ã¦ã€å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚")
    
    prompt_text = "\n".join(text_parts)
    
    return prompt_text, image_documents


def query_with_multimodal(
    index, 
    query_text: str, 
    similarity_top_k: int = 3,
    max_tokens: int = 2000,
    image_detail: str = "high",
    max_images: int = 5,
    response_mode: str = "compact",
    use_chat_history: bool = True,
    chat_history_length: int = 5,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    seed: int = None
) -> Dict[str, Any]:
    """
    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
    """
    logger.info(f"Executing multimodal query: {query_text[:50]}...")
    
    try:
        # é€šå¸¸ã®RAGæ¤œç´¢ã§ãƒãƒ¼ãƒ‰ã‚’å–å¾—
        query_engine = index.as_query_engine(
            similarity_top_k=similarity_top_k,
            response_mode=response_mode
        )
        
        response = query_engine.query(query_text)
        source_nodes = response.source_nodes
        
        # ç”»åƒã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—
        image_cache = st.session_state.image_cache
        
        # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt_text, image_documents = create_multimodal_prompt(
            query_text, 
            source_nodes,
            image_cache
        )
        
        # GPT-4 Visionã‚’ä½¿ç”¨ï¼ˆç”»åƒãŒã‚ã‚‹å ´åˆï¼‰
        if image_documents:
            logger.info(f"Using GPT-4 Vision with {len(image_documents)} images")
            
            # ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
            chat_history = []
            if use_chat_history and "messages" in st.session_state:
                recent_messages = st.session_state.messages[-(chat_history_length * 2):]
                for msg in recent_messages:
                    if msg["role"] in ("user", "assistant"):
                        chat_history.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })
            
            # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            image_base64_list = [
                image_to_base64(img_doc["image"])
                for img_doc in image_documents[:max_images]
            ]
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’æ§‹ç¯‰
            messages = build_chat_messages(
                prompt_text=prompt_text,
                image_base64_list=image_base64_list,
                image_detail=image_detail,
                max_images=max_images,
                chat_history=chat_history,
            )
            
            # GPT-4 Vision APIå‘¼ã³å‡ºã—
            llm_model = st.session_state.get("llm_model", "gpt-4o-mini")
            temperature = st.session_state.get("temperature", 0.1)
            
            answer_text = call_chat_api(
                messages=messages,
                model=llm_model,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                seed=seed,
            )
            
            logger.info(f"Vision query: images={len(image_base64_list)}, detail={image_detail}, history={use_chat_history}({chat_history_length if use_chat_history else 0} turns)")
            
        else:
            # ç”»åƒãŒãªã„å ´åˆã‚‚ openai_client çµŒç”±ã§çµ±ä¸€
            logger.info("No images found, using standard LLM via openai_client")
            
            # ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
            chat_history = []
            if use_chat_history and "messages" in st.session_state:
                recent_messages = st.session_state.messages[-(chat_history_length * 2):]
                for msg in recent_messages:
                    if msg["role"] in ("user", "assistant"):
                        chat_history.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })
            
            messages = build_chat_messages(
                prompt_text=prompt_text,
                image_base64_list=[],
                chat_history=chat_history,
            )
            
            llm_model = st.session_state.get("llm_model", "gpt-4o-mini")
            temperature = st.session_state.get("temperature", 0.1)
            
            answer_text = call_chat_api(
                messages=messages,
                model=llm_model,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                seed=seed,
            )
        
        return {
            "answer": answer_text,
            "source_nodes": source_nodes,
            "image_documents": image_documents,
            "success": True
        }
    
    except Exception as e:
        logger.error(f"Multimodal query failed: {e}", exc_info=True)
        return {
            "answer": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "source_nodes": [],
            "image_documents": [],
            "success": False,
            "error": str(e)
        }


def render_response_with_images(answer: str, image_documents: List[Dict]):
    """
    å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã¦ã€ç”»åƒå‚ç…§éƒ¨åˆ†ã«å®Ÿéš›ã®ç”»åƒã‚’åŸ‹ã‚è¾¼ã‚€
    """
    import re
    
    # [ç”»åƒ1]ã€[ç”»åƒ2]ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    pattern = r'\[ç”»åƒ(\d+)\]'
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    parts = re.split(pattern, answer)
    
    for i, part in enumerate(parts):
        if i % 2 == 0:
            # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†
            if part.strip():
                st.markdown(part)
        else:
            # ç”»åƒç•ªå·
            img_num = int(part)
            # è©²å½“ã™ã‚‹ç”»åƒã‚’è¡¨ç¤ºï¼ˆã‚¯ãƒªãƒƒã‚¯ã§æ‹¡å¤§å¯èƒ½ãªã‚µãƒ ãƒã‚¤ãƒ«ï¼‰
            for img_doc in image_documents:
                if img_doc["number"] == img_num:
                    with st.expander(f"ğŸ–¼ï¸ ç”»åƒ{img_num}: {img_doc['metadata'].get('file_name')} - Page {img_doc['metadata'].get('page')}"):
                        st.image(
                            img_doc["image"],
                            width=480,
                        )
                    break
